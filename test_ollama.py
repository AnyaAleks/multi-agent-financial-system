# test_ollama.py
import requests
import json
from langchain_community.chat_models import ChatOllama
from langchain.schema import HumanMessage


def test_ollama_connection():
    """–¢–µ—Å—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Ollama"""
    print("üîç –¢–µ—Å—Ç–∏—Ä—É—é —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Ollama...")

    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ API
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            models = response.json().get("models", [])
            print(f"‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç")
            print(f"üì¶ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: {[m['name'] for m in models]}")
            return True
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")
        return False


def test_llama_model():
    """–¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ Llama"""
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä—É—é –º–æ–¥–µ–ª—å Llama 3.1...")

    try:
        # –°–æ–∑–¥–∞–µ–º LLM –∫–ª–∏–µ–Ω—Ç
        llm = ChatOllama(
            model="llama3.1:latest",
            temperature=0.1,
            base_url="http://localhost:11434"
        )

        # –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
        response = llm.invoke([
            HumanMessage(content="–ü—Ä–∏–≤–µ—Ç! –û—Ç–≤–µ—Ç—å –ø—Ä–æ—Å—Ç–æ '–¢–µ—Å—Ç –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ'")
        ])

        print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç: {response.content[:100]}...")
        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏: {e}")
        return False


def test_financial_analysis():
    """–¢–µ—Å—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    print("\nüìä –¢–µ—Å—Ç–∏—Ä—É—é —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑...")

    try:
        llm = ChatOllama(
            model="llama3.1:latest",
            temperature=0.1
        )

        prompt = """
        –¢—ã —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∞–∫—Ü–∏–∏ Apple (AAPL).
        –î–∞–π –∫—Ä–∞—Ç–∫—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é: BUY, HOLD –∏–ª–∏ SELL.
        –û–±—ä—è—Å–Ω–∏ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–æ–π.
        """

        response = llm.invoke([HumanMessage(content=prompt)])

        print(f"‚úÖ –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–±–æ—Ç–∞–µ—Ç:")
        print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {response.content}")
        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return False


def main():
    print("=" * 50)
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Financial Analysis System —Å Ollama")
    print("=" * 50)

    # –¢–µ—Å—Ç 1: –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
    if not test_ollama_connection():
        print("\n‚ö†Ô∏è  Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve")
        return

    # –¢–µ—Å—Ç 2: –ú–æ–¥–µ–ª—å
    if not test_llama_model():
        print("\n‚ö†Ô∏è  –ü—Ä–æ–±–ª–µ–º–∞ —Å –º–æ–¥–µ–ª—å—é. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: ollama pull llama3.1")
        return

    # –¢–µ—Å—Ç 3: –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
    test_financial_analysis()

    print("\n" + "=" * 50)
    print("üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã! –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ.")
    print("=" * 50)

    print("\nüìã –î–∞–ª–µ–µ:")
    print("1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –¥–∞—à–±–æ—Ä–¥: python scripts/dev_start.py")
    print("2. –ò–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –∞–Ω–∞–ª–∏–∑: python -c \"from test_ollama import *; main()\"")


if __name__ == "__main__":
    main()